{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29_9LXSvFbH-",
        "outputId": "2c5f1a33-7ba9-4197-fd38-ae86e9ba7a3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yKvXrEk_F2cN",
        "outputId": "20303c79-b513-4921-eda9-bac1b94468f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcZSXp6GGCh9",
        "outputId": "ed1f0920-eef3-4082-e690-9acb5bc5cfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "w2v = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86u6P6EBVNS1",
        "outputId": "1e121b23-20e2-4cc8-f139-d846bff5f296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m781.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=c76bd7350ad47d91e9850761f9e1b15ed475e44f801e2244c81253209d11820d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VqyD0ibpZua"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVVFjxR64C68"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.layers import TimeDistributed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzVxAHAN4HC8"
      },
      "source": [
        "**Question 1.1**  \n",
        "Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar\n",
        "word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word\n",
        "and its cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl_V9Fi-GSNt",
        "outputId": "0d4e9c98-8be8-4012-da21-1249149cf914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(a): ('students', 0.7294867038726807)\n",
            "(b): ('Apple_AAPL', 0.7456986308097839)\n",
            "(c): ('apples', 0.720359742641449)\n"
          ]
        }
      ],
      "source": [
        "# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models\n",
        "# https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
        "print(\"(a):\", w2v.most_similar('student')[0])\n",
        "print(\"(b):\", w2v.most_similar('Apple')[0])\n",
        "print(\"(c):\", w2v.most_similar('apple')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51unsnX14fR1"
      },
      "source": [
        "**Question 1.2**  \n",
        "(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
        "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
        "etc.) you chose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHkk5-aeIGxx"
      },
      "outputs": [],
      "source": [
        "testaPath = keras.utils.get_file(\n",
        "    fname=\"eng.testa\",\n",
        "    origin=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/data/eng.testa\",\n",
        "    extract=True,\n",
        ")\n",
        "testbPath =  keras.utils.get_file(\n",
        "    fname=\"eng.testb\",\n",
        "    origin=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/data/eng.testb\",\n",
        "    extract=True,\n",
        ")\n",
        "trainPath =  keras.utils.get_file(\n",
        "    fname=\"eng.train\",\n",
        "    origin=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/data/eng.train\",\n",
        "    extract=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT_UMAqePGH1"
      },
      "outputs": [],
      "source": [
        "def getData(path):\n",
        "  with open(path, 'r') as file:\n",
        "    dataset = file.read()\n",
        "  lines = dataset.split('\\n')\n",
        "  split_lines = [line.split() for line in lines]\n",
        "  max_words = max(len(line) for line in split_lines)\n",
        "  split_lines = [line + [''] * (max_words - len(line)) for line in split_lines]\n",
        "  dataset = pd.DataFrame(split_lines)\n",
        "  dataset.columns = [f'col{i}' for i in range(max_words)]\n",
        "  dataset = dataset.drop(columns=['col1', 'col2'])\n",
        "  dataset.rename(columns={'col0': 'word', 'col3': 'label'}, inplace=True)\n",
        "  return dataset\n",
        "\n",
        "lastRow = {'word': '', 'label': ''}\n",
        "lastRow_df = pd.DataFrame([lastRow])\n",
        "\n",
        "train = getData(trainPath)\n",
        "development = getData(testaPath)\n",
        "development = pd.concat([development, lastRow_df], ignore_index=True)\n",
        "test = getData(testbPath)\n",
        "\n",
        "category_mapping = {'': 0,'I-LOC': 1, 'B-ORG': 2, 'I-ORG': 3, 'I-PER': 4, 'B-LOC': 5, 'I-MISC': 6, 'B-MISC': 7, 'O': 8}\n",
        "train[\"encoded_label\"] = train[\"label\"].map(category_mapping)\n",
        "development[\"encoded_label\"] = development[\"label\"].map(category_mapping)\n",
        "test[\"encoded_label\"] = test[\"label\"].map(category_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeXEeeT5UxJB",
        "outputId": "8a26d3be-01da-4057-973a-592eace854f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              word  label  encoded_label\n",
            "219549     Lincoln  I-ORG              3\n",
            "219550           2      O              8\n",
            "219551                                 0\n",
            "219552  -DOCSTART-      O              8\n",
            "219553                                 0\n",
            "               word  label  encoded_label\n",
            "55039      Newsroom  I-ORG              3\n",
            "55040  880-2-506363      O              8\n",
            "55041                                   0\n",
            "55042    -DOCSTART-      O              8\n",
            "55043                                   0\n",
            "             word  label  encoded_label\n",
            "50345       Bobby  I-PER              4\n",
            "50346           .      O              8\n",
            "50347                                 0\n",
            "50348  -DOCSTART-      O              8\n",
            "50349                                 0\n"
          ]
        }
      ],
      "source": [
        "print(train.tail())\n",
        "print(development.tail())\n",
        "print(test.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf8UjKmKPYBL",
        "outputId": "27a6b2d8-a937-46b9-9cb2-cc7dd914af8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training file:  14987\n",
            "Size of development file:  3466\n",
            "Size of test file:  3684\n",
            "Complete Set of all word labels (IOB1): ['', 'B-ORG', 'I-PER', 'I-MISC', 'I-LOC', 'O', 'I-ORG', 'B-LOC', 'B-MISC']\n"
          ]
        }
      ],
      "source": [
        "def countSize(df):\n",
        "  return (df.applymap(lambda x: x == '').any(axis=1)).sum()\n",
        "\n",
        "def getCompleteLabelSet(train, development, test):\n",
        "    label_set = set(train[\"label\"].unique()) | set(development[\"label\"].unique()) | set(test[\"label\"].unique())\n",
        "    return list(label_set)\n",
        "\n",
        "print(\"Size of training file: \", countSize(train))\n",
        "print(\"Size of development file: \", countSize(development))\n",
        "print(\"Size of test file: \", countSize(test))\n",
        "\n",
        "print(\"Complete Set of all word labels (IOB1):\", getCompleteLabelSet(train, development, test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dep6uV9q5o"
      },
      "source": [
        "(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
        "entities with more than one word. Explain how to form complete named entities from the label\n",
        "for each word, and list all the named entities in this sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN77PfCY9e3m"
      },
      "outputs": [],
      "source": [
        "def getNamedEntitiesForEachSentence(df, namedEntitylabels = ['I-LOC', 'B-ORG', 'I-ORG', 'I-PER', 'B-LOC', 'I-MISC', 'B-MISC']):\n",
        "  targetSentences = []\n",
        "  sentenceArray = []\n",
        "  NECount = 0\n",
        "  MWNECount = 0\n",
        "  NEWordCount = 0\n",
        "  NEArray = []\n",
        "  NE = \"\"\n",
        "  previousNELabel = None\n",
        "\n",
        "  for index,row in df.iterrows():\n",
        "    # if word is a period, it symbolises the end of a sentence\n",
        "    if row[\"word\"] == \"\":\n",
        "      # evaluate the previous named entity if present\n",
        "      if NEWordCount > 1:\n",
        "        MWNECount += 1\n",
        "        NECount += 1\n",
        "        NEArray.append(NE)\n",
        "\n",
        "      # save other single word named entities\n",
        "      elif NEWordCount == 1 :\n",
        "        NECount += 1\n",
        "        NEArray.append(NE)\n",
        "      sentenceArray.append([row[\"word\"], row[\"label\"]])\n",
        "\n",
        "      # for each Sentence, check if the number of Multi-Word Named Entities is >= 2\n",
        "      if NECount >= 2:\n",
        "        # if yes, append to targetSentences\n",
        "        targetSentences.append([sentenceArray,NEArray,NECount, MWNECount])\n",
        "      # set all variables to process the next sentence\n",
        "      sentenceArray = []\n",
        "      NECount = 0\n",
        "      MWNECount = 0\n",
        "      NEWordCount = 0\n",
        "      NEArray = []\n",
        "      NE = \"\"\n",
        "      previousLabel = None\n",
        "      continue\n",
        "\n",
        "    # if word belongs has a named entity label\n",
        "    elif row[\"label\"] in namedEntitylabels:\n",
        "      # get the label type and the position\n",
        "      position, label = row[\"label\"].split(\"-\")\n",
        "      # if a word with the same label has appeared before\n",
        "      # and the current word is not beginning word\n",
        "      # the previous and current words can be grouped togther as 1 named entity\n",
        "      if previousNELabel is not None and previousNELabel == label and position != \"B\":\n",
        "        NEWordCount += 1\n",
        "        NE += row[\"word\"] + \" \"\n",
        "\n",
        "      else:\n",
        "        # if the current word is the start of a new named entity\n",
        "        # evaluate the previous named entity\n",
        "        if NEWordCount > 1:\n",
        "          NECount += 1\n",
        "          MWNECount += 1\n",
        "          NEArray.append(NE)\n",
        "\n",
        "        # save other single word named entities\n",
        "        elif NEWordCount == 1 :\n",
        "          NECount += 1\n",
        "          NEArray.append(NE)\n",
        "\n",
        "        #and track the current new named entity\n",
        "        previousNELabel = label\n",
        "        NEWordCount = 1\n",
        "        NE = \"\"\n",
        "        NE += row[\"word\"]+\" \"\n",
        "\n",
        "    # if word does not have a named entity label\n",
        "    else:\n",
        "      # if there was a named enitity before current word,\n",
        "      # evaluate it\n",
        "      if NEWordCount > 1:\n",
        "        MWNECount += 1\n",
        "        NECount += 1\n",
        "        NEArray.append(NE)\n",
        "      elif NEWordCount == 1:\n",
        "        NECount += 1\n",
        "        NEArray.append(NE)\n",
        "\n",
        "      #reset values\n",
        "      NE = \"\"\n",
        "      NEWordCount = 0\n",
        "\n",
        "    sentenceArray.append([row[\"word\"],row[\"label\"]])\n",
        "\n",
        "  return targetSentences\n",
        "\n",
        "def getSentenceFromRowArray(array):\n",
        "  sentence = \"\"\n",
        "  for word, label in array:\n",
        "    sentence += word\n",
        "    sentence += \" \"\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J65VFlf0Fo1D"
      },
      "outputs": [],
      "source": [
        "sentences = getNamedEntitiesForEachSentence(train)\n",
        "sentences = sorted(sentences, key=lambda x: x[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A66bIRxGcTf",
        "outputId": "654fb0e3-0d4b-464f-d176-8a35e57dfad0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6136"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDtj352r33vZ"
      },
      "outputs": [],
      "source": [
        "# a = []\n",
        "# for i in range(0,len(sentences)):\n",
        "#   for j in sentences[i][0]:\n",
        "#     if j[1].split(\"-\")[0] == \"B\" and sentences[i][3] > 0:\n",
        "#       a.append(i)\n",
        "# a = list(set(a))\n",
        "\n",
        "# for i in a:\n",
        "#   print(i)\n",
        "#   print(\"Example Sentence:\", getSentenceFromRowArray(sentences[i][0]))\n",
        "#   print(\"All named entities:\",sentences[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ4Mn8mPse-S",
        "outputId": "c20411ca-f51f-49c1-8084-c099e8556c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Sentence: Sosa , a leading candidate for National League Most Valuable Player honours , was injured August 20th when he was hit by a Mark Hutton pitch in the first inning of an 8-1 victory over the Florida Marlins .  \n",
            "All named entities: ['Sosa ', 'National League ', 'Most Valuable Player ', 'Mark Hutton ', 'Florida Marlins ']\n",
            "\n",
            "Consecutive entity labels with the same class (e.g PER) can be grouped together to get a named entity. B tags in the label can be used to split a chunk of entity words into meaningful groups.\n",
            "\n",
            "Labels (for reference): [['Sosa', 'I-PER'], [',', 'O'], ['a', 'O'], ['leading', 'O'], ['candidate', 'O'], ['for', 'O'], ['National', 'I-MISC'], ['League', 'I-MISC'], ['Most', 'B-MISC'], ['Valuable', 'I-MISC'], ['Player', 'I-MISC'], ['honours', 'O'], [',', 'O'], ['was', 'O'], ['injured', 'O'], ['August', 'O'], ['20th', 'O'], ['when', 'O'], ['he', 'O'], ['was', 'O'], ['hit', 'O'], ['by', 'O'], ['a', 'O'], ['Mark', 'I-PER'], ['Hutton', 'I-PER'], ['pitch', 'O'], ['in', 'O'], ['the', 'O'], ['first', 'O'], ['inning', 'O'], ['of', 'O'], ['an', 'O'], ['8-1', 'O'], ['victory', 'O'], ['over', 'O'], ['the', 'O'], ['Florida', 'I-ORG'], ['Marlins', 'I-ORG'], ['.', 'O'], ['', '']] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "random = 6041\n",
        "print(\"Example Sentence:\", getSentenceFromRowArray(sentences[random][0]))\n",
        "print(\"All named entities:\",sentences[random][1])\n",
        "\n",
        "print(\"\\nConsecutive entity labels with the same class (e.g PER) can be grouped together to get a named entity. B tags in the label can be used to split a chunk of entity words into meaningful groups.\\n\")\n",
        "\n",
        "print(\"Labels (for reference):\",sentences[random][0], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUdBkL8KA1Dx"
      },
      "source": [
        "**Question 1.3**  \n",
        "(a) Discuss how you deal with new words in the training set which are not found in the pretrained\n",
        "dictionary. Likewise, how do you deal with new words in the test set which are not found in\n",
        "either the pretrained dictionary or the training set? Show the corresponding code snippet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YPLq2_s439q"
      },
      "outputs": [],
      "source": [
        "def getSentenceAndVocab(df):\n",
        "  allSentences = []\n",
        "  sentenceArray = []\n",
        "  newVocab = {}\n",
        "  keyValue = 0\n",
        "\n",
        "  for index,row in df.iterrows():\n",
        "    if row[\"word\"] == \"\":\n",
        "      sentenceArray.append([row[\"word\"], row[\"encoded_label\"]])\n",
        "      allSentences.append(sentenceArray)\n",
        "      sentenceArray = []\n",
        "      continue\n",
        "    else:\n",
        "      try:\n",
        "        w2v[row[\"word\"]]\n",
        "      except:\n",
        "          try:\n",
        "              newVocab[row[\"word\"]]\n",
        "          except:\n",
        "              newVocab[row[\"word\"]] = keyValue\n",
        "              keyValue += 1\n",
        "\n",
        "    sentenceArray.append([row[\"word\"],row[\"encoded_label\"]])\n",
        "\n",
        "  return allSentences, newVocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73qd8CuU4575"
      },
      "outputs": [],
      "source": [
        "trainSentences, trainVocab = getSentenceAndVocab(train)\n",
        "developmentSentences, developmentVocab = getSentenceAndVocab(development)\n",
        "testSentences, testVocab = getSentenceAndVocab(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvNOdqmQ5CKE"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "\n",
        "def getNgrams(train):\n",
        "  ngramsDict = {}\n",
        "  for idx, row in train.iterrows():\n",
        "    try:\n",
        "      w2v[row[\"word\"]]\n",
        "    except:\n",
        "      continue\n",
        "    if row[\"word\"] not in ngramsDict:\n",
        "      ngramsDict[row[\"word\"]] = list(ngrams(row[\"word\"], 3)) + list(ngrams(row[\"word\"], 2))\n",
        "  return ngramsDict\n",
        "\n",
        "def getMostSimilarKeys(OOVWord, ngramsDict):\n",
        "  OOVNgrams = list(ngrams(OOVWord, 3)) + list(ngrams(OOVWord, 2))\n",
        "  similarKeys = []\n",
        "  keyCount = []\n",
        "  for n in ngramsDict.keys():\n",
        "    currentCount = len(set(OOVNgrams).intersection(set(ngramsDict[n])))\n",
        "    keyCount.append([n,currentCount, -len(ngramsDict[n])])\n",
        "\n",
        "  keyCount = sorted(keyCount, key = lambda x: (x[1], x[2]))\n",
        "\n",
        "  for value in keyCount[-3:]:\n",
        "    similarKeys.append(value[0])\n",
        "  return similarKeys\n",
        "\n",
        "def getOOVVector(similarKeys, OOVWord, vectorDim = 300):\n",
        "  random_vectors = []\n",
        "\n",
        "  np.random.seed(seed = 42)\n",
        "  random_vectors.append((np.random.rand(vectorDim) * 2.0) - 1.0)\n",
        "\n",
        "  random_vector = np.mean(random_vectors, axis=0)\n",
        "  random_vector = random_vector / np.linalg.norm(random_vector)\n",
        "\n",
        "  sum_vector = 0\n",
        "  for word in similarKeys:\n",
        "    sum_vector += w2v[word]\n",
        "  mean_vector =  np.divide(sum_vector, len(similarKeys))\n",
        "  final_vector = (random_vector * 0.3 + mean_vector * 0.7)\n",
        "  final_vector = final_vector / np.linalg.norm(final_vector)\n",
        "\n",
        "  return final_vector\n",
        "\n",
        "def embedNewWordsNgrams(trainVocab, ngramsDict):\n",
        "  for key, value in trainVocab.items():\n",
        "    similarKeys = getMostSimilarKeys(key, ngramsDict)\n",
        "    trainVocab[key] = getOOVVector(similarKeys, key)\n",
        "\n",
        "\n",
        "ngramsDictTrain = getNgrams(train)\n",
        "embedNewWordsNgrams(trainVocab, ngramsDictTrain)\n",
        "\n",
        "ngramsDictVal = getNgrams(development)\n",
        "embedNewWordsNgrams(developmentVocab, ngramsDictVal)\n",
        "\n",
        "ngramsDictTest = getNgrams(test)\n",
        "embedNewWordsNgrams(testVocab, ngramsDictTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuPmoIiP5SVp",
        "outputId": "b967b0c4-33d4-4f44-a195-2000b804de0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125\n"
          ]
        }
      ],
      "source": [
        "maxLen = max(max([len(s) for s in trainSentences]),\n",
        "                  max([len(s) for s in developmentSentences]),\n",
        "                  max([len(s) for s in testSentences]))\n",
        "print(maxLen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgZxOPn05T0z"
      },
      "outputs": [],
      "source": [
        "# 300 is the size of vector embedding\n",
        "xTrainArray = np.zeros((len(trainSentences), maxLen, 300))\n",
        "xDevelopmentArray = np.zeros((len(developmentSentences), maxLen, 300))\n",
        "xTestArray = np.zeros((len(testSentences), maxLen, 300))\n",
        "yTrainArray = np.zeros((len(trainSentences), maxLen))\n",
        "yDevelopmentArray = np.zeros((len(developmentSentences), maxLen))\n",
        "yTestArray = np.zeros((len(testSentences), maxLen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtpQrDry5Usl"
      },
      "outputs": [],
      "source": [
        "def createXArray(sentences, array, vocab):\n",
        "  for sentence_idx, sentence in enumerate(sentences):\n",
        "      arrayPos = 0  # Initialize the array position for each sentence\n",
        "      for word in sentence:\n",
        "          if arrayPos >= maxLen: # truncate\n",
        "              break  # If the sentence exceeds the maximum length, exit the loop\n",
        "          if word[0] == \"\":\n",
        "              for i in range(arrayPos, maxLen): # padding\n",
        "                  array[sentence_idx][i] = 0\n",
        "          else:\n",
        "              try: # check whether it is in pretrained word2vec\n",
        "                  array[sentence_idx][arrayPos] = w2v[word[0]]\n",
        "              except: # check whether it is in vocab\n",
        "                  array[sentence_idx][arrayPos] = vocab[word[0]]\n",
        "          arrayPos += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzsIY7Pl5XdE"
      },
      "outputs": [],
      "source": [
        "def createYArray(sentences, array):\n",
        "    for sentence_idx, sentence in enumerate(sentences):\n",
        "        arrayPos = 0  # Initialize the array position for each sentence\n",
        "        for word in sentence:\n",
        "            if arrayPos >= maxLen:\n",
        "                break  # If the sentence exceeds the maximum length, exit the loop\n",
        "            if word[0] == \"\":\n",
        "                for i in range(arrayPos, maxLen):\n",
        "                    array[sentence_idx][i] = 0\n",
        "            else:\n",
        "                array[sentence_idx][arrayPos] = word[1]\n",
        "            arrayPos += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kdruf2X5XCZ"
      },
      "outputs": [],
      "source": [
        "createXArray(trainSentences, xTrainArray, trainVocab)\n",
        "createYArray(trainSentences, yTrainArray)\n",
        "\n",
        "createXArray(developmentSentences, xDevelopmentArray, developmentVocab)\n",
        "createYArray(developmentSentences, yDevelopmentArray)\n",
        "\n",
        "createXArray(testSentences, xTestArray, testVocab)\n",
        "createYArray(testSentences, yTestArray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIW8ZsiC5gnh",
        "outputId": "81d909ad-eeb4-4ced-9447-7ef8e04ee3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 125, 300)]        0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 125, 300)          721200    \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 125, 9)            2709      \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 723909 (2.76 MB)\n",
            "Trainable params: 723909 (2.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input = Input(shape=(maxLen, 300))\n",
        "model = LSTM(units=300,return_sequences=True)(input)\n",
        "output = TimeDistributed(Dense(9, activation=\"softmax\"))(model)\n",
        "model = Model(input,output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5zsLA6cyZVO"
      },
      "outputs": [],
      "source": [
        "# Do yall want to try bidirectional? Should have better perf. Not sure if it will work\n",
        "# https://www.kaggle.com/code/rahulkumarpatro/named-entity-recognition-using-lstm\n",
        "from tensorflow.keras.layers import SpatialDropout1D, Bidirectional\n",
        "input = Input(shape=(maxLen, 300))\n",
        "model = SpatialDropout1D(0.2)(input)\n",
        "model = Bidirectional(LSTM(units=150,return_sequences=True, recurrent_dropout=0.2))(input)\n",
        "output = TimeDistributed(Dense(9, activation=\"softmax\"))(model)\n",
        "model = Model(input,output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyu2_IrbTKBu"
      },
      "source": [
        "(b) Describe what neural network you used to produce the final vector representation of each\n",
        "word and what are the mathematical functions used for the forward computation (i.e., from\n",
        "the pretrained word vectors to the final label of each word). Give the detailed setting of the\n",
        "network including which parameters are being updated, what are their sizes, and what is the\n",
        "length of the final vector representation of each word to be fed to the softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpAMsRnXntrs"
      },
      "outputs": [],
      "source": [
        "units=300\n",
        "W = model.layers[1].get_weights()[0]\n",
        "U = model.layers[1].get_weights()[1]\n",
        "b = model.layers[1].get_weights()[2]\n",
        "\n",
        "W_i = W[:, :units]\n",
        "W_f = W[:, units: units * 2]\n",
        "W_c = W[:, units * 2: units * 3]\n",
        "W_o = W[:, units * 3:]\n",
        "\n",
        "U_i = U[:, :units]\n",
        "U_f = U[:, units: units * 2]\n",
        "U_c = U[:, units * 2: units * 3]\n",
        "U_o = U[:, units * 3:]\n",
        "\n",
        "b_i = b[:units]\n",
        "b_f = b[units: units * 2]\n",
        "b_c = b[units * 2: units * 3]\n",
        "b_o = b[units * 3:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekc3gjkksaaB",
        "outputId": "94c030a9-ebb9-4817-a3b1-1df80d759599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights for the forget gate:\n",
            "W_f:  (300, 300)\n",
            "U_f:  (300, 300)\n",
            "b_f:  (300,)\n",
            "Weights for the input gate:\n",
            "W_i:  (300, 300)\n",
            "U_i:  (300, 300)\n",
            "b_i:  (300,)\n",
            "Weights for the output gate:\n",
            "W_o:  (300, 300)\n",
            "U_o:  (300, 300)\n",
            "b_o:  (300,)\n",
            "Weights for the candidate cell gate:\n",
            "W_c:  (300, 300)\n",
            "U_c:  (300, 300)\n",
            "b_c:  (300,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Weights for the forget gate:\")\n",
        "print(\"W_f: \", W_f.shape)\n",
        "print(\"U_f: \", U_f.shape)\n",
        "print(\"b_f: \", b_f.shape)\n",
        "print(\"Weights for the input gate:\")\n",
        "print(\"W_i: \", W_i.shape)\n",
        "print(\"U_i: \", U_i.shape)\n",
        "print(\"b_i: \", b_i.shape)\n",
        "print(\"Weights for the output gate:\")\n",
        "print(\"W_o: \", W_o.shape)\n",
        "print(\"U_o: \", U_o.shape)\n",
        "print(\"b_o: \", b_o.shape)\n",
        "print(\"Weights for the candidate cell gate:\")\n",
        "print(\"W_c: \", W_c.shape)\n",
        "print(\"U_c: \", U_c.shape)\n",
        "print(\"b_c: \", b_c.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP-AWfSoTNIZ"
      },
      "source": [
        "(c) Report how many epochs you used for training, as well as the running time.\n",
        "\n",
        "Used 3 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5i2W8KHTZ3n"
      },
      "source": [
        "(d) Report the f1 score on the test set, as well as the f1 score on the development set for each\n",
        "epoch during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh6UOl8OURxW"
      },
      "outputs": [],
      "source": [
        "class F1Callback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def __init__(self, x_dev, y_dev, id2label, pad_value=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            id2label (dict): id to label mapping.\n",
        "            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n",
        "            pad_value (int): padding value.\n",
        "        \"\"\"\n",
        "        super(F1Callback, self).__init__()\n",
        "        self.id2label = id2label\n",
        "        self.pad_value = pad_value\n",
        "        self.x_dev = x_dev\n",
        "        self.y_dev = y_dev\n",
        "\n",
        "    def find_pad_index(self, array):\n",
        "        \"\"\"Find padding index.\n",
        "        Args:\n",
        "            array (list): integer list.\n",
        "        Returns:\n",
        "            idx: padding index.\n",
        "        Examples:\n",
        "             >>> array = [1, 2, 0]\n",
        "             >>> self.find_pad_index(array)\n",
        "             2\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return list(array).index(self.pad_value)\n",
        "        except ValueError:\n",
        "            return len(array)\n",
        "    def get_length(self, y):\n",
        "        \"\"\"Get true length of y.\n",
        "        Args:\n",
        "            y (list): padded list.\n",
        "        Returns:\n",
        "            lens: true length of y.\n",
        "        Examples:\n",
        "            >>> y = [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\n",
        "            >>> self.get_length(y)\n",
        "            [1, 2, 3]\n",
        "        \"\"\"\n",
        "        lens = [self.find_pad_index(row) for row in y]\n",
        "        return lens\n",
        "\n",
        "    def convert_idx_to_name(self, y, lens):\n",
        "        \"\"\"Convert label index to name.\n",
        "        Args:\n",
        "            y (list): label index list.\n",
        "            lens (list): true length of y.\n",
        "        Returns:\n",
        "            y: label name list.\n",
        "        Examples:\n",
        "            >>> # assumes that id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "            >>> y = [[1, 0, 0], [1, 2, 0], [1, 1, 1]]\n",
        "            >>> lens = [1, 2, 3]\n",
        "            >>> self.convert_idx_to_name(y, lens)\n",
        "            [['B-LOC'], ['B-LOC', 'I-LOC'], ['B-LOC', 'B-LOC', 'B-LOC']]\n",
        "        \"\"\"\n",
        "        y = [[self.id2label[idx] for idx in row[0:l-1]]\n",
        "             for row, l in zip(y, lens)]\n",
        "        return y\n",
        "    def predict(self, X, y):\n",
        "        \"\"\"Predict sequences.\n",
        "        Args:\n",
        "            X (list): input data.\n",
        "            y (list): tags.\n",
        "        Returns:\n",
        "            y_true: true sequences.\n",
        "            y_pred: predicted sequences.\n",
        "        \"\"\"\n",
        "        y_pred = self.model.predict(xDevelopmentArray)\n",
        "\n",
        "        # reduce dimension.\n",
        "        y_true = np.argmax(y, -1)\n",
        "        y_pred = np.argmax(y_pred, -1)\n",
        "\n",
        "        lens = self.get_length(y_true)\n",
        "\n",
        "        y_true = self.convert_idx_to_name(y_true, lens)\n",
        "        y_pred = self.convert_idx_to_name(y_pred, lens)\n",
        "\n",
        "        return y_true, y_pred\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Predict on the development set\n",
        "        y_pred = self.model.predict(xDevelopmentArray)\n",
        "\n",
        "        # Convert one-hot encoded labels to class indices\n",
        "        y_true = self.y_dev\n",
        "        y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "        lens = self.get_length(y_true)\n",
        "        print(lens)\n",
        "        y_true = self.convert_idx_to_name(y_true, lens)\n",
        "        y_pred = self.convert_idx_to_name(y_pred, lens)\n",
        "\n",
        "        score = f1_score(y_true, y_pred)\n",
        "\n",
        "        print(' - f1: {:04.2f}'.format(score * 100))\n",
        "        print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "        epoch_duration = time.time() - self.start_time\n",
        "        print(f\"Epoch {epoch + 1} took {epoch_duration} seconds\")\n",
        "\n",
        "labels = {0:'Pd',1:'I-LOC', 2:'B-ORG', 3:'I-ORG', 4:'I-PER', 5:'B-LOC', 6:'I-MISC', 7:'B-MISC', 8:'O'}\n",
        "f1score = F1Callback(x_dev = xDevelopmentArray, y_dev = yDevelopmentArray,id2label= labels )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYHzzA8I5kRd"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "524K3zHp5lHl"
      },
      "outputs": [],
      "source": [
        "history=model.fit(xTrainArray,yTrainArray,batch_size=32,epochs=3,callbacks=[f1score])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLu1j05R5nha"
      },
      "outputs": [],
      "source": [
        "devPreds = model.predict(xDevelopmentArray)\n",
        "testPreds = model.predict(xTestArray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haayVjKy5oT0"
      },
      "outputs": [],
      "source": [
        "def getTagsForEachSentence(predictedLabels, sentences):\n",
        "    outputArray = []\n",
        "    for i in range(0, len(sentences)):\n",
        "        for j in range(0, len(sentences[i])):\n",
        "            outputArray.append(predictedLabels[i][j])\n",
        "\n",
        "    return outputArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqzP9PEQ9VBY"
      },
      "outputs": [],
      "source": [
        "devLabels = np.argmax(devPreds, axis=-1)\n",
        "print(classification_report(getTagsForEachSentence(devLabels, developmentSentences), getTagsForEachSentence(yDevelopmentArray, developmentSentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6d2_pAS934E"
      },
      "outputs": [],
      "source": [
        "testLabels = np.argmax(testPreds, axis=-1)\n",
        "print(classification_report(getTagsForEachSentence(testLabels, testSentences), getTagsForEachSentence(yTestArray, testSentences)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
